
# ğŸ¯ Term Deposit Subscription Prediction â€” Bank Marketing Campaign

Predict whether a customer will subscribe to a **term deposit** based on a direct marketing campaign using ML techniques and explainable AI.

---

## ğŸ“ Dataset

* **Source**: [UCI Machine Learning Repository - Bank Marketing Dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing)
* **Records**: 41,188
* **Target Variable**: `y` (Yes/No â€” term deposit subscription)

---

## ğŸ“Œ Objectives

âœ… Predict whether a bank client will subscribe to a term deposit
âœ… Encode and preprocess the dataset properly
âœ… Train and evaluate classification models
âœ… Use Explainable AI (SHAP) for interpreting predictions
âœ… Compare model performance and choose the best

---

## ğŸ› ï¸ Workflow

### 1. ğŸ“Š Data Exploration

* Checked for null values and data types
* Visualized class imbalance
* Explored correlations

### 2. ğŸ§¹ Preprocessing

* Label encoded the target variable
* One-hot encoded categorical features
* Scaled numerical values (where needed)

### 3. ğŸ§  Models Trained

* Logistic Regression
* Random Forest Classifier
* XGBoost Classifier

### 4. ğŸ“ˆ Evaluation Metrics

* Confusion Matrix
* Precision, Recall, F1-Score
* ROC-AUC Curve

### 5. ğŸ§  Explainability

* Used **SHAP** to explain **5 individual predictions** from the best model
* Plotted SHAP summary for global feature importance

---

## ğŸ“Š Model Performance Summary

| Model         | Accuracy | Precision (1) | Recall (1) | F1-Score (1) | AUC  |
| ------------- | -------- | ------------- | ---------- | ------------ | ---- |
| Logistic Reg. | 0.9007   | 0.70          | 0.21       | 0.32         | 0.80 |
| Random Forest | 0.8978   | 0.59          | **0.31**   | **0.40**     | 0.80 |
| XGBoost       | 0.8997   | 0.62          | 0.29       | 0.39         | 0.80 |

> ğŸ” **Accuracy â‰ˆ 90%** across all models, but since the dataset is imbalanced, **F1-Score and Recall for class 1 (Yes)** are more meaningful.

---

## âœ… Confusion Matrix (for Class `1` - Subscription)

| Model         | TP (1)  | FN  | FP  | TN   |
| ------------- | ------- | --- | --- | ---- |
| Logistic Reg. | 191     | 737 | 81  | 7229 |
| Random Forest | **286** | 642 | 200 | 7110 |
| XGBoost       | 266     | 662 | 164 | 7146 |

* ğŸ”¸ **Random Forest** catches **most true positives** (286), making it better for capturing actual subscribers.
* ğŸ”¸ It also has the **lowest false negatives**, reducing missed "Yes" predictions.

---

## ğŸ§  Model Interpretability with SHAP

Used SHAP (SHapley Additive exPlanations) to:

* Visualize **feature importance**
* Explain **5 individual predictions** with local interpretation plots
* Identify key factors influencing "Yes" and "No" predictions

> ğŸ’¡ Top impactful features: `duration`, `month`, `poutcome`, `previous`, `campaign`

---

## ğŸ“Š Final Comparison Table: Random Forest vs XGBoost (Before & After SMOTE)

| Metric                  | RF Before SMOTE | RF After SMOTE | XGB Before SMOTE | XGB After SMOTE |
| ----------------------- | --------------- | -------------- | ---------------- | --------------- |
| **Accuracy**            | âœ… 0.8978        | ğŸ”» 0.8881      | âœ… 0.8997         | âœ… 0.8999        |
| **AUC Score**           | âœ… 0.79          | ğŸ”» 0.78        | âœ… 0.80           | ğŸ”º **0.8035**   |
| **Recall (Class 1)**    | 0.31            | ğŸ”º **0.37**    | 0.29             | ğŸ”º 0.34         |
| **Precision (Class 1)** | âœ… 0.59          | ğŸ”» 0.50        | âœ… 0.62           | ğŸ”» 0.60         |
| **F1-Score (Class 1)**  | 0.40            | ğŸ”º **0.43**    | 0.39             | ğŸ”º **0.43**     |
| **FN (Conf. Matrix)**   | 573             | 617            | 662              | ğŸ”º **617**      |
| **Support (Class 1)**   | 928             | 928            | 928              | 928             |

---

## ğŸ§  Analysis: Which is Better & Why?

### âœ… **XGBoost After SMOTE is the Best Choice**

Here's **why**:

| Criteria                     | Explanation                                                                |
| ---------------------------- | -------------------------------------------------------------------------- |
| ğŸ“ˆ **Balanced F1-Score**     | XGBoost After SMOTE achieves **0.43**, equal to RF After SMOTE â€” but...    |
| ğŸ“Š **Better AUC Score**      | XGBoost has **0.8035 AUC**, higher than RFâ€™s 0.78 â€” meaning better ranking |
| ğŸ¯ **Improved Recall**       | Recall improves from 0.29 â†’ 0.34 (helps detect more positive cases)        |
| ğŸ§ª **Stable Accuracy**       | Accuracy remains **almost unchanged** after SMOTE (0.8997 â†’ 0.8999)        |
| ğŸ¤– **More Robust to SMOTE**  | Unlike RF, XGBoost did **not lose much precision or accuracy**             |
| ğŸ” **Fewer False Negatives** | FN dropped from 662 â†’ 617 in XGBoost, matching RF but with better AUC      |

---

## ğŸ§¾ Justification Text:

> After applying SMOTE to handle class imbalance, we evaluated Random Forest and XGBoost using multiple metrics â€” accuracy, AUC, precision, recall, F1-score, and confusion matrix.
>
> While both models showed improvement in detecting the minority class, **XGBoost After SMOTE emerged as the most balanced and robust model**. It achieved the **highest AUC (0.8035)**, stable accuracy (0.8999), and **better recall and F1-score** without significantly compromising precision.
>
> Therefore, **XGBoost with SMOTE was chosen** as the final model due to its superior balance between overall performance and minority class detection, which is crucial for tasks like mental health prediction where false negatives are costly.

---


## ğŸ Final Verdict

> âœ… **Random Forest** is the **best overall model** based on:
>
> * Highest **Recall** and **F1-score** for minority class (1)
> * Most **True Positives** in Confusion Matrix
> * Balanced trade-off between precision and recall
> * Reliable performance and interpretability using SHAP

---

## ğŸ“š Skills Demonstrated

* âœ… Data preprocessing & encoding
* âœ… Binary classification modeling
* âœ… Model evaluation with imbalanced data
* âœ… Explainable AI (XAI) with SHAP
* âœ… Customer behavior analytics

---

## ğŸ“ Repository Structure

```
ğŸ“¦ Term-Deposit-Prediction/
â”‚
â”œâ”€â”€ ğŸ“ notebook/   
â”‚   â”œâ”€â”€ ğŸ“„ term_deposit_prediction.ipynb       # Complete notebook with all steps
â”‚   â””â”€â”€ ğŸ“„ README.md                            # Notebook-specific readme
â”‚
â”œâ”€â”€ ğŸ“ dataset/                         
â”‚   â”œâ”€â”€ ğŸ“„ bank-additional-full.csv            # Dataset file
â”‚   â””â”€â”€ ğŸ“„ README.md                            # Dataset description
â”‚
â”œâ”€â”€ ğŸ“„ encoder.pkl                              # Encoder used for categorical feature transformation
â”œâ”€â”€ ğŸ“„ scaler.pkl                               # Scaler used for feature normalization
â”œâ”€â”€ ğŸ“„ xgb_term_deposit_model.pkl              # Trained XGBoost model for prediction
â”‚
â””â”€â”€ ğŸ“„ README.md                                # Main project readme (this file)


```



## ğŸš€ Project Highlights

* âœ… 3 models trained, evaluated, and compared
* âœ… SHAP-based interpretability used
* âœ… Real-world imbalanced classification problem tackled

---

## ğŸ¤ Contact

**Muhammad Zain Mushtaq**
AI/ML & Data Science Enthusiast | Researcher
ğŸ“¬ [LinkedIn](https://www.linkedin.com/in/muhammad-zain-m-a75163358/) | ğŸ“§ [mzainmushtaq@gmail.com](mailto:mzainmushtaq@gmail.com)

---

## ğŸŒŸ If you like this project, give it a â­ and share!
